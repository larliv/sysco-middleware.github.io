I"À)<h1 id="introduction">Introduction</h1>

<p>Letâ€™s talk about Kafka Schema Registry essentials.</p>

<h1 id="repository">Repository</h1>

<p>Repository contains examples of schema evolution - full compatibility.
<a href="https://github.com/sysco-middleware/poc-confluent-schema-registry-p1">poc-confluent-schema-registry-p1</a></p>

<h2 id="content">Content:</h2>

<ol>
  <li>What is Confluent Schema Registry and why we need it?<br />
1.1. Use case.<br />
1.2. Compatibility types.<br />
1.3. Avro serializer.</li>
  <li>How to define schema?<br />
2.1. In source code.<br />
2.2. Using reflection.<br />
2.3. Using json.</li>
  <li>Schema evolution example.<br />
3.1. Up kafka.<br />
3.2. Up project.
3.2. Up project.</li>
  <li>Important notes.</li>
</ol>

<h2 id="1-what-is-confluent-schema-registry-and-why-we-need-it">1. What is Confluent Schema Registry and why we need it?</h2>

<p>Confluent Schema Registry is application, which manage compatibility and provides RESTful interface to preform CRUD operations. 
Schemas can be applied to key/value or both.</p>

<p><code class="highlighter-rouge">!NB</code><a href="https://github.com/confluentinc/schema-registry/pull/680">issue680</a> 
Kafka producer will accept any mixture of Avro record types and publish them to the same topic.</p>

<p>Confluent schema registry is separate node. Kafka Consumer/Producer should have <code class="highlighter-rouge">schema.registry.url</code> and specific <code class="highlighter-rouge">serializer/deserializer</code> in properties, 
if schema registry is in use.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Properties properties = new Properties();
  properties.setProperty("schema.registry.url", "http://localhost:8081");
</code></pre></div></div>

<p>Confluent schema registry should be fault tolerant. 
If node with schema registry not available, all clients will fail while producing/consuming (if no caching on clientâ€™s sides).<br />
Kafka itself is not responsible for data verification, it stores only bytes and publish them.</p>

<p><img src="/images/2018-07-13-ConfluentKafkaSchemaRegistryPart1/confluent_schema_registry.png" alt="Confluent Schema registry" />
<a href="https://medium.com/@stephane.maarek/introduction-to-schemas-in-apache-kafka-with-the-confluent-schema-registry-3bf55e401321">Source</a></p>

<h3 id="11-use-case">1.1. Use case</h3>

<p>All of us faced with continuously coming new requirements from customers. 
Sometimes we need to change data transfer objects: add additional fields or remove some of them,<br />
which will cause mapping. 
Schema registry is answer to - how to support schema versions and achieve full compatibility. 
Do all those changes without breaking any dependent parts.</p>

<h3 id="12-compatibility-types">1.2. Compatibility types</h3>

<table>
  <thead>
    <tr>
      <th>Type</th>
      <th style="text-align: center">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Backward</td>
      <td style="text-align: center">Old schema can be used to read New data</td>
    </tr>
    <tr>
      <td>Forward</td>
      <td style="text-align: center">New schema can be used to read Old data</td>
    </tr>
    <tr>
      <td>Full compatibility (Forward and Backward)</td>
      <td style="text-align: center">Both (Old &amp; New) schemas can be used to read old &amp; new data</td>
    </tr>
    <tr>
      <td>Breaking</td>
      <td style="text-align: center">None of those</td>
    </tr>
  </tbody>
</table>

<p>Target is type <code class="highlighter-rouge">Full compatibility</code>.</p>

<p><img src="/images/2018-07-13-ConfluentKafkaSchemaRegistryPart1/full_compatibility.png" alt="Full compatibility" /></p>

<p>Full compatibility means that message which is produced with Old-schema can be consumed with New-Schema 
and opposite, message which is produced with New-schema can be consumed using Old-Schema.</p>

<h3 id="13-avro-serializer">1.3. Avro serializer.</h3>

<p><a href="https://github.com/apache/avro">Apache Avro</a> is data serializer. This serializer is often used with
Confluent Schema Registry for Kafka.</p>

<p>Avro key-features:</p>
<ul>
  <li>Compressed data</li>
  <li><a href="https://avro.apache.org/docs/1.8.2/spec.html#schemas">Types support</a></li>
  <li>Embedded documentation support</li>
  <li>Defined using json</li>
  <li>Avro object contains schema and data</li>
  <li>Shema evolution</li>
</ul>

<h2 id="2-how-to-define-schema">2. How to define schema?</h2>

<p>There are 3 ways how to define avro schema.</p>

<h3 id="21-in-source-code">2.1. In source code.</h3>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Schema.Parser parser = new Schema.Parser();
Schema schema = parser.parse("{\n"
  + "   \"type\": \"record\",\n"
  + "   \"namespace\": \"no.sysco.middleware.poc.kafkaschemaregistry.avro\",\n"
  + "   \"name\": \"Business\",\n"
  + "   \"version\": \"1\",\n"
  + "   \"doc\":\"Business record contains name of company and list of customers\",\n"
  + "   \"fields\": [\n"
  + "     { \"name\": \"company_name\", \"type\": \"string\", \"doc\": \"Name of company\" },\n"
  + "     {\n"
  + "        \"name\": \"customers\",\n"
  + "        \"doc\": \"List of customers\",\n"
  + "        \"type\": {\n"
  + "          \"type\": \"array\",\n"
  + "          \"items\": {\n"
  + "            \"type\": \"record\",\n"
  + "            \"name\":\"Customer\",\n"
  + "            \"fields\":[\n"
  + "              { \"name\": \"first_name\", \"type\": \"string\", \"doc\":\"Customer name\" },\n"
  + "              { \"name\": \"last_name\", \"type\": \"string\", \"doc\": \"Customer last name\" }\n"
  + "            ]\n"
  + "          }\n"
  + "        }\n"
  + "     }\n"
  + "   ]\n"
  + "}");
</code></pre></div></div>

<p><a href="https://github.com/simplesteph/kafka-avro-course-udemy/blob/master/avro-examples/src/main/java/com/github/simplesteph/avro/generic/GenericRecordExamples.java">Full example</a></p>

<h3 id="22-using-reflection">2.2. Using reflection.</h3>

<p>Use reflection. Make schema from POJO class. Do not forget to create empty constructor.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Schema schema = ReflectData.get().getSchema(ReflectedCustomer.class);
</code></pre></div></div>

<p><a href="https://github.com/simplesteph/kafka-avro-course-udemy/blob/master/avro-examples/src/main/java/com/github/simplesteph/avro/reflection/ReflectedCustomer.java">Pojo class example</a><br />
<a href="https://github.com/simplesteph/kafka-avro-course-udemy/blob/master/avro-examples/src/main/java/com/github/simplesteph/avro/reflection/ReflectionExamples.java">Reflection full example</a></p>

<h3 id="23-using-json">2.3. Using json.</h3>

<p>The way is to define schema in file <code class="highlighter-rouge">.avsc</code>. Definitionâ€™s syntax is <code class="highlighter-rouge">json</code>.</p>

<p><a href="https://mvnrepository.com/artifact/org.apache.avro/avro-maven-plugin">Avro-maven-plugin</a> generates classes from <code class="highlighter-rouge">.avsc</code>.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
   "type": "record",
   "namespace": "no.sysco.middleware.poc.kafkaschemaregistry.avro",
   "name": "Business",
   "version": "1",
   "doc":"Business record contains name of company and list of customers",
   "fields": [
     { "name": "company_name", "type": "string", "doc": "Name of company" },
     {
        "name": "customers",
        "doc": "List of customers",
        "type": {
          "type": "array",
          "items": {
            "type": "record",
            "name":"Customer",
            "fields":[
              { "name": "first_name", "type": "string", "doc":"Customer name" },
              { "name": "last_name", "type": "string", "doc": "Customer last name" }
            ]
          }
        }
     }
   ]
}
</code></pre></div></div>

<h2 id="3-schema-evolution">3. Schema evolution.</h2>

<h3 id="up-kafka">Up kafka</h3>

<p>Kafka should be <code class="highlighter-rouge">UP</code>, before perform next steps. There are two options how to do it:</p>
<ol>
  <li><code class="highlighter-rouge">Landoop</code> - kafka cluster using ladoop images: with UI on <a href="http://localhost:3030">http://localhost:3030</a>, Connectors and additional tools.<br />
<code class="highlighter-rouge">docker-compose -f docker-compose.landoop.yml up -d</code></li>
  <li><code class="highlighter-rouge">Confluent</code> - kafka cluster using confluent images<br />
<code class="highlighter-rouge">docker-compose -f docker-compose.confluent.yml up -d</code></li>
</ol>

<h3 id="evolution">Evolution</h3>

<p>Build project and generate classes from <a href="https://github.com/sysco-middleware/kafka-schema-registry-poc/blob/master/version1/src/main/resources/avro/business-v1.avsc">business-v1.avsc</a> and <a href="https://github.com/sysco-middleware/kafka-schema-registry-poc/blob/master/version2/src/main/resources/avro/business-v2.avsc">business-v2.avsc</a>.</p>

<p><code class="highlighter-rouge">./mvnw clean package</code></p>

<p>Run producers in modules <code class="highlighter-rouge">version1</code> and <code class="highlighter-rouge">version2</code> in any sequential order to produce messages to one topic. 
Run consumers in modules <code class="highlighter-rouge">version1</code> and <code class="highlighter-rouge">version2</code> in any sequential order to see consumption results.</p>

<h2 id="4-important-notes">4. Important notes.</h2>

<p>These important notes help to achieve full compatibility</p>
<ol>
  <li>Make your primary key required</li>
  <li>Give default values to all the fields that could be removed in the future</li>
  <li>Be very careful when using ENUM as the can not evolve over time</li>
  <li>Do not rename fields. You can add aliases instead</li>
  <li>When evolving schema, ALWAYS give default values</li>
  <li>When evolving schema, NEVER remove, rename of the required field or change the type</li>
</ol>

<h2 id="refs--sources">Refs &amp; sources</h2>

<ul>
  <li><a href="https://docs.confluent.io/current/schema-registry/docs/index.html">Schema Registry doc</a></li>
  <li><a href="https://avro.apache.org/docs/1.8.2/index.html">Apache Avro doc 1.8.2</a></li>
  <li><a href="https://medium.com/@stephane.maarek/introduction-to-schemas-in-apache-kafka-with-the-confluent-schema-registry-3bf55e401321">Stephane Maarek</a></li>
</ul>

<h2 id="next-posts">Next posts</h2>

<ul>
  <li>More Complex examples</li>
  <li>Confluent Schema Registry with Kafka Streams API</li>
  <li>REST Proxy Schema registry</li>
</ul>
:ET